################
# General XCSF #
################

OMP_NUM_THREADS=8 # number of threads for parallel processing
POP_SIZE=2000 # maximum number of macro-classifiers in the population
MAX_TRIALS=100000 # number of learning trials to perform
POP_INIT=true # whether to fill the initial population with random classifiers
PERF_TRIALS=1000 # number of trials to average performance output
LOSS_FUNC=mae # Mean Absolute Error loss function (use for mazes and mux)
#LOSS_FUNC=mse # Mean Squared Error
#LOSS_FUNC=rmse # Root Mean Squared Error
#LOSS_FUNC=log # Log loss; for multi-class classification
#LOSS_FUNC=binary-log # Binary log loss; for binary-class classification
#LOSS_FUNC=onehot # One-hot encoding classification error
#LOSS_FUNC=huber # Huber loss
HUBER_DELTA=1 # Delta parameter for Huber loss calculation

#########################
# Multi-step parameters #
#########################

TELETRANSPORTATION=50 # num steps to reset a multistep problem if goal not found
GAMMA=0.95 # discount factor in calculating the reward for multistep problems
P_EXPLORE=0.9 # probability of exploring vs. exploiting in a multistep trial

######################
# General Classifier #
######################

E0=0.01 # classifier target error, under which accuracy is set to 1
ALPHA=0.1 # classifier accuracy offset for rules above E0 (1=disabled)
NU=5.0 # classifier accuracy slope for rules with error above E0
BETA=0.1 # learning rate for updating error, fitness, and set size
DELTA=0.1 # fraction of least fit classifiers to increase deletion vote
THETA_DEL=20 # min experience before fitness used in probability of deletion
INIT_FITNESS=0.01 # initial classifier fitness
INIT_ERROR=0.0 # initial classifier error
M_PROBATION=10000 # trials since creation a rule must match at least 1 input or be deleted
STATEFUL=true # whether classifiers should retain state across trials
SET_SUBSUMPTION=false # whether to perform set subsumption
THETA_SUB=100 # minimum experience of a classifier to become a subsumer
COMPACTION=false # if enabled and sys err < E0, the largest of 2 roulette spins is deleted
 
##########################
# Evolutionary Algorithm #
##########################

EA_SELECT_TYPE=roulette # roulette wheel parental selection
#EA_SELECT_TYPE=tournament # tournament parental selection
EA_SELECT_SIZE=0.4 # fraction of set size for tournament parental selection
THETA_EA=50.0 # average set time between EA invocations
LAMBDA=2 # number of offspring to create each EA invocation
P_CROSSOVER=0.8 # probability of applying crossover
ERR_REDUC=1.0 # amount to reduce an offspring's error (1=disabled)
FIT_REDUC=0.1 # amount to reduce an offspring's fitness (1=disabled)
EA_SUBSUMPTION=false # whether to try and subsume offspring classifiers
EA_PRED_RESET=false # whether to reset offspring predictions instead of copying
 
########################
# Classifier Condition #
########################

#COND_TYPE=dummy # always matching dummy condition
COND_TYPE=hyperrectangle # hyperrectangles
#COND_TYPE=hyperellipsoid # hyperellipsoids
#COND_TYPE=neural # neural networks
#COND_TYPE=tree-gp # GP Trees
#COND_TYPE=dgp # dynamical GP graphs
#COND_TYPE=ternary # ternary strings (binarises inputs)
#COND_TYPE=rule-dgp # both conditions and actions in single dynamical GP graphs
#COND_TYPE=rule-neural # both conditions and actions in single neural networks

# Hyperrectangles and hyperellipsoids
COND_MIN=0.0 # minimum input value
COND_MAX=1.0 # maximum input value
COND_SPREAD_MIN=0.1 # minimum initial spread
COND_ETA=0.0 # gradient descent rate for moving centers to mean inputs matched

# Ternary
COND_BITS=1 # bits per float to binarise inputs for ternary conditions (mp=1, maze=2 or 3)
COND_P_DONTCARE=0.5 # don't care probability for ternary conditions

# Tree-GP
COND_GP_NUM_CONS=100 # number of (shared) constants available for GP trees 
COND_GP_INIT_DEPTH=5 # initial depth of GP trees
COND_GP_MAX_LEN=10000 # maximum initial length of GP trees
COND_GP_MIN_CON=0 # minimum value of a GP tree constant
COND_GP_MAX_CON=1 # maximum value of a GP tree constant

# DGP
COND_DGP_MAX_K=2 # maximum number of connections a DGP node may have
COND_DGP_MAX_T=10 # maximum number of cycles to update a DGP graph
COND_DGP_N=20 # number of nodes in a DGP graph
COND_DGP_EVOLVE_CYCLES=true # whether to evolve the number of update cycles

# Neural network: each layer in sequence (input -> output) must start with LAYER_TYPE
COND_LAYER_TYPE=connected
LAYER_ACTIVATION=logistic
LAYER_N_INIT=10
LAYER_N_MAX=100
LAYER_EVOLVE_WEIGHTS=true
LAYER_EVOLVE_CONNECT=true
LAYER_EVOLVE_NEURONS=true
LAYER_EVOLVE_FUNCTIONS=false
LAYER_MAX_NEURON_GROW=1

COND_LAYER_TYPE=connected
LAYER_ACTIVATION=linear
LAYER_EVOLVE_WEIGHTS=true
LAYER_EVOLVE_CONNECT=true
LAYER_EVOLVE_FUNCTIONS=false

#####################
# Classifier Action #
#####################

ACT_TYPE=integer # integers (use this for regression / function approximation)
#ACT_TYPE=neural # neural networks

# Neural network: each layer in sequence (input -> output) must start with LAYER_TYPE
ACT_LAYER_TYPE=connected
LAYER_ACTIVATION=selu
LAYER_N_INIT=1
LAYER_N_MAX=100
LAYER_EVOLVE_WEIGHTS=true
LAYER_EVOLVE_CONNECT=true
LAYER_EVOLVE_NEURONS=true
LAYER_EVOLVE_FUNCTIONS=false
LAYER_MAX_NEURON_GROW=1

ACT_LAYER_TYPE=connected
LAYER_ACTIVATION=linear
LAYER_EVOLVE_WEIGHTS=true
LAYER_EVOLVE_CONNECT=true
LAYER_EVOLVE_FUNCTIONS=false

ACT_LAYER_TYPE=softmax
LAYER_SCALE=1

#########################
# Classifier Prediction #
#########################

#PRED_TYPE=constant # Constant
PRED_TYPE=nlms-linear # linear least squares
#PRED_TYPE=nlms-quadratic # quadratic least squares
#PRED_TYPE=rls-linear # linear recursive least squares
#PRED_TYPE=rls-quadratic # quadratic recursive least squares
#PRED_TYPE=neural # stochastic gradient descent multilayer perceptron neural networks

# Least squares
PRED_ETA=0.1 # gradient descent rate for updating predictions (max value if evolved)
PRED_ETA_MIN=0.00001 # minimum gradient descent rate (if evolved)
PRED_EVOLVE_ETA=true # whether to use self-adaptive mutation to set eta
PRED_X0=1.0 # prediction weight vector offset value
PRED_RLS_SCALE_FACTOR=1000.0 # initial diagonal values of the RLS gain-matrix
PRED_RLS_LAMBDA=1.0 # forget rate for RLS (small values may be unstable)

# Neural network: each layer in sequence (input -> output) must start with LAYER_TYPE
PRED_LAYER_TYPE=connected
LAYER_ACTIVATION=logistic
LAYER_N_INIT=10
LAYER_N_MAX=100
LAYER_EVOLVE_WEIGHTS=true
LAYER_EVOLVE_CONNECT=true
LAYER_EVOLVE_NEURONS=true
LAYER_EVOLVE_FUNCTIONS=false
LAYER_MAX_NEURON_GROW=1
LAYER_EVOLVE_ETA=true
LAYER_SGD_WEIGHTS=true
LAYER_ETA=0.1
LAYER_ETA_MIN=0.0001
LAYER_MOMENTUM=0.9
LAYER_DECAY=0

PRED_LAYER_TYPE=connected
LAYER_ACTIVATION=logistic
LAYER_EVOLVE_WEIGHTS=true
LAYER_EVOLVE_CONNECT=true
LAYER_EVOLVE_FUNCTIONS=false
LAYER_EVOLVE_ETA=true
LAYER_SGD_WEIGHTS=true
LAYER_ETA=0.1
LAYER_ETA_MIN=0.0001
LAYER_MOMENTUM=0.9
LAYER_DECAY=0
